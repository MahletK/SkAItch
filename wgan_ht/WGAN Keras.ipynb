{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # enable hi-res output\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils.generic_utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND = 777\n",
    "\n",
    "RUN = 'B'\n",
    "OUT_DIR = 'out/' + RUN\n",
    "TENSORBOARD_DIR = './tensorboard/wgans/' + RUN\n",
    "SAVE_SAMPLE_IMAGES = False\n",
    "\n",
    "# GPU # to run on\n",
    "# GPU = \"0\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "ITERATIONS = 200\n",
    "\n",
    "# size of the random vector used to initialize G\n",
    "Z_SIZE = 100\n",
    "\n",
    "D_ITERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(RND)\n",
    "\n",
    "# force Keras to use last dimension for image channels\n",
    "#K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_D():\n",
    "    \n",
    "    # weights are initlaized from normal distribution with below params\n",
    "    weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "    input_image = Input(shape=(28, 28, 1), name='input_image')\n",
    "\n",
    "    x = Conv2D(\n",
    "        32, (3, 3),\n",
    "        padding='same',\n",
    "        name='conv_1',\n",
    "        kernel_initializer=weight_init)(input_image)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        64, (3, 3),\n",
    "        padding='same',\n",
    "        name='conv_2',\n",
    "        kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        128, (3, 3),\n",
    "        padding='same',\n",
    "        name='conv_3',\n",
    "        kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        256, (3, 3),\n",
    "        padding='same',\n",
    "        name='coonv_4',\n",
    "        kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    features = Flatten()(x)\n",
    "\n",
    "    output_is_fake = Dense(\n",
    "        1, activation='linear', name='output_is_fake')(features)\n",
    "\n",
    "    output_class = Dense(\n",
    "        10, activation='softmax', name='output_class')(features)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[input_image], outputs=[output_is_fake, output_class], name='D')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_G(Z_SIZE=Z_SIZE):\n",
    "    \n",
    "    DICT_LEN = 10\n",
    "    EMBEDDING_LEN = Z_SIZE\n",
    "\n",
    "    # weights are initialized from normal distribution with below params\n",
    "    weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "    # class#\n",
    "    input_class = Input(shape=(1, ), dtype='int32', name='input_class')\n",
    "    \n",
    "    e = Embedding(\n",
    "        DICT_LEN, EMBEDDING_LEN,\n",
    "        embeddings_initializer='glorot_uniform')(input_class)\n",
    "    embedded_class = Flatten(name='embedded_class')(e)\n",
    "\n",
    "    # latent var\n",
    "    input_z = Input(shape=(Z_SIZE, ), name='input_z')\n",
    "\n",
    "    # hadamard product\n",
    "    h = multiply([input_z, embedded_class], name='h')\n",
    "    \n",
    "    x = Dense(512)(h)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(64 * 7 * 7)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((7, 7, 64))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        1, (2, 2),\n",
    "        padding='same',\n",
    "        activation='tanh',\n",
    "        name='output_generated_image',\n",
    "        kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=[input_z, input_class], outputs=x, name='G')\n",
    "#     model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_z_ (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_class_ (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "G (Model)                       (None, 28, 28, 1)    2071529     input_z_[0][0]                   \n",
      "                                                                 input_class_[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "D (Model)                       [(None, 1), (None, 1 525835      G[1][0]                          \n",
      "==================================================================================================\n",
      "Total params: 2,597,364\n",
      "Trainable params: 2,597,364\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = create_D()\n",
    "\n",
    "D.compile(\n",
    "    optimizer=RMSprop(lr=0.00005),\n",
    "    loss=[d_loss, 'sparse_categorical_crossentropy'])\n",
    "\n",
    "input_z = Input(shape=(Z_SIZE, ), name='input_z_')\n",
    "input_class = Input(shape=(1, ),name='input_class_', dtype='int32')\n",
    "\n",
    "G = create_G()\n",
    "\n",
    "# create combined D(G) model\n",
    "output_is_fake, output_class = D(G(inputs=[input_z, input_class]))\n",
    "DG = Model(inputs=[input_z, input_class], outputs=[output_is_fake, output_class])\n",
    "DG.summary()\n",
    "\n",
    "DG.compile(\n",
    "    optimizer=RMSprop(lr=0.00005),\n",
    "    loss=[d_loss, 'sparse_categorical_crossentropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# use all available 70k samples from both train and test sets\n",
    "# X_train = np.concatenate((X_train, X_test))\n",
    "# y_train = np.concatenate((y_train, y_test))\n",
    "X_train = X_train[:6000]\n",
    "y_train = y_train[:6000]\n",
    "\n",
    "# convert to -1..1 range, reshape to (sample_i, 28, 28, 1)\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n=0, save=True):\n",
    "\n",
    "    zz = np.random.normal(0., 1., (100, Z_SIZE))\n",
    "    generated_classes = np.array(list(range(0, 10)) * 10)\n",
    "    generated_images = G.predict([zz, generated_classes.reshape(-1, 1)])\n",
    "\n",
    "    rr = []\n",
    "    for c in range(10):\n",
    "        rr.append(\n",
    "            np.concatenate(generated_images[c * 10:(1 + c) * 10]).reshape(\n",
    "                280, 28))\n",
    "    img = np.hstack(rr)\n",
    "\n",
    "    if save:\n",
    "        plt.imsave(OUT_DIR + '/samples_%07d.png' % n, img, cmap=plt.cm.gray)\n",
    "\n",
    "    return img\n",
    "\n",
    "# write tensorboard summaries\n",
    "sw = tf.summary.FileWriter(TENSORBOARD_DIR)\n",
    "def update_tb_summary(step, write_sample_images=True):\n",
    "\n",
    "    s = tf.Summary()\n",
    "\n",
    "    # losses as is\n",
    "    for names, vals in zip((('D_real_is_fake', 'D_real_class'),\n",
    "                            ('D_fake_is_fake', 'D_fake_class'), ('DG_is_fake',\n",
    "                                                                 'DG_class')),\n",
    "                           (D_true_losses, D_fake_losses, DG_losses)):\n",
    "\n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][1]\n",
    "        v.tag = names[0]\n",
    "\n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][2]\n",
    "        v.tag = names[1]\n",
    "\n",
    "    # D loss: -1*D_true_is_fake - D_fake_is_fake\n",
    "    v = s.value.add()\n",
    "    v.simple_value = -D_true_losses[-1][1] - D_fake_losses[-1][1]\n",
    "    v.tag = 'D loss (-1*D_real_is_fake - D_fake_is_fake)'\n",
    "\n",
    "    # generated image\n",
    "    if write_sample_images:\n",
    "        img = generate_samples(step, save=True)\n",
    "        s.MergeFromString(tf.Session().run(\n",
    "            tf.summary.image('samples_%07d' % step,\n",
    "                             img.reshape([1, *img.shape, 1]))))\n",
    "\n",
    "    sw.add_summary(s, step)\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "  0/200 [..............................] - ETA: 0s\tDITER0\n",
      "\tDITER1\n",
      "\tDITER2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ht/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "  1/200 [..............................] - ETA: 2:21:37 - D_real_is_fake: -9.9070e-04 - D_real_class: 2.3026 - D_fake_is_fake: 0.0014 - D_fake_class: 2.3026 - D(G)_is_fake: -3.6984e-04 - D(G)_class: 2.3026\tDITER0\n",
      "\tDITER1\n",
      "\tDITER2\n",
      "Iteration: 2\n",
      "  2/200 [..............................] - ETA: 2:14:03 - D_real_is_fake: -0.0024 - D_real_class: 2.3025 - D_fake_is_fake: 0.0021 - D_fake_class: 2.3026 - D(G)_is_fake: -8.9503e-04 - D(G)_class: 2.3026    \tDITER0\n",
      "\tDITER1\n",
      "\tDITER2\n",
      "Iteration: 3\n",
      "  3/200 [..............................] - ETA: 2:10:50 - D_real_is_fake: -0.0051 - D_real_class: 2.3025 - D_fake_is_fake: 0.0034 - D_fake_class: 2.3026 - D(G)_is_fake: -0.0016 - D(G)_class: 2.3026    \tDITER0\n",
      "\tDITER1\n"
     ]
    }
   ],
   "source": [
    "progress_bar = Progbar(target=ITERATIONS)\n",
    "\n",
    "DG_losses = []\n",
    "D_true_losses = []\n",
    "D_fake_losses = []\n",
    "\n",
    "for it in range(ITERATIONS):\n",
    "    print(\"Iteration: {}\".format(it))\n",
    "\n",
    "    if len(D_true_losses) > 0:\n",
    "        progress_bar.update(\n",
    "            it,\n",
    "            values=[ # avg of 5 most recent\n",
    "                    ('D_real_is_fake', np.mean(D_true_losses[-5:], axis=0)[1]),\n",
    "                    ('D_real_class', np.mean(D_true_losses[-5:], axis=0)[2]),\n",
    "                    ('D_fake_is_fake', np.mean(D_fake_losses[-5:], axis=0)[1]),\n",
    "                    ('D_fake_class', np.mean(D_fake_losses[-5:], axis=0)[2]),\n",
    "                    ('D(G)_is_fake', np.mean(DG_losses[-5:],axis=0)[1]),\n",
    "                    ('D(G)_class', np.mean(DG_losses[-5:],axis=0)[2])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        progress_bar.update(it)\n",
    "\n",
    "    # 1: train D on real+generated images\n",
    "\n",
    "#     if (it % 1000) < 25 or it % 500 == 0: # 25 times in 1000, every 500th\n",
    "#         d_iters = 100\n",
    "#     else:\n",
    "    d_iters = D_ITERS\n",
    "\n",
    "    for d_it in range(d_iters):\n",
    "#         print(\"\\tDITER{}\".format(d_it))\n",
    "\n",
    "        # unfreeze D\n",
    "        D.trainable = True\n",
    "        for l in D.layers: l.trainable = True\n",
    "\n",
    "        # clip D weights\n",
    "\n",
    "        for l in D.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "        # 1.1: maximize D output on reals === minimize -1*(D(real))\n",
    "\n",
    "        # draw random samples from real images\n",
    "        index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "        real_images = X_train[index]\n",
    "        real_images_classes = y_train[index]\n",
    "\n",
    "        D_loss = D.train_on_batch(real_images, [-np.ones(BATCH_SIZE), \n",
    "          real_images_classes])\n",
    "        D_true_losses.append(D_loss)\n",
    "\n",
    "        # 1.2: minimize D output on fakes \n",
    "\n",
    "        zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE))\n",
    "        generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "        generated_images = G.predict([zz, generated_classes.reshape(-1, 1)])\n",
    "\n",
    "        D_loss = D.train_on_batch(generated_images, [np.ones(BATCH_SIZE),\n",
    "          generated_classes])\n",
    "        D_fake_losses.append(D_loss)\n",
    "\n",
    "    # 2: train D(G) (D is frozen)\n",
    "    # minimize D output while supplying it with fakes, \n",
    "    # telling it that they are reals (-1)\n",
    "\n",
    "    # freeze D\n",
    "    D.trainable = False\n",
    "    for l in D.layers: l.trainable = False\n",
    "\n",
    "    zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE)) \n",
    "    generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "\n",
    "    DG_loss = DG.train_on_batch(\n",
    "        [zz, generated_classes.reshape((-1, 1))],\n",
    "        [-np.ones(BATCH_SIZE), generated_classes])\n",
    "\n",
    "    DG_losses.append(DG_loss)\n",
    "\n",
    "    if it % 1 == 0:\n",
    "        update_tb_summary(it, write_sample_images=(it % 250 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
